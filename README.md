# RSLModels.jl


## Generate samples for later configuring learning task generator: `scripts/genkdata.jl`


```
nix develop --impure
julia -p 20 --project=. scripts/genkdata.jl --n-iter 10000
```


See also `slurm/genkdata.sbatch` (which can be submitted to our Slurm cluster
using `sbatch slurm/genkdata.sbatch`). Adjust the number of processes (`-p`
option to `julia`) according to the hardware you use.


This generates a CSV file such as `2023-11-23-12-23-35-578-Bym3-kdata.csv`.
Let's assume that this file's name is `$kdata_csv`.


If you have access to a Slurm cluster, consider to start many medium-sized jobs
(e.g. 1000 jobs with `--n-iter 100` each). Put the files generated into a new
directory and use that directory as `$kdata_csv` in the next step instead of a
single file name.


## Extract learning task generator hyperparameters: `scripts/selectgendataparams.jl`


```
nix develop --impure
julia --project=. scripts/selectgendataparams.jl $kdata_csv
```


Replace the CSV file name with the name of the file generated by `scripts/genkdata.jl`.


## Generate learning tasks: `scripts/gendata.jl`

```
nix develop --impure
julia --project=. -e "import Pkg; Pkg.instantiate()"
julia -p 30 --project=. "scripts/gendata.jl" genall --startseed=0 --endseed=49 --prefix-fname="data" --full --usemmap
```

Remember to adjust the parameter to `-p` to the actual number of workers you want to use.


Also, if you don't want to use mmap (because you have a lot of RAM) then don't
add the `--usemmap` flag. You probably want to do that, though, especially for
higher-than-ten dimensional data.


## Selecting a subset of the generated tasks: `scripts/drawdata.jl`


Since it's not (yet) a proper CLI app, you will have to adjust the file names at
the top of `scripts/drawdata.jl`.


```
nix develop --impure
julia --project=. scripts/drawdata.jl
```


Run the fish script generated by `drawdata.jl` in order to copy the selected
tasks into a folder.


## Check ranges of tuned parameters (i.e. the results of `run.py optparams`): `scripts/chkoptparams.jl`


For now, the interface is to adjust in the script the path to the mlruns folder
to check.


## Compute dissimilarities (can takes a long time, run this on a compute node): `scripts/computesims.jl`


This serializes a DataFrame with all the mlflow data (including the
dissimilarities) for the next step.


## Peform analysis of the run set: `scripts/analyserunbest.jl`


You probably want to run this on the server since computing the pairwise
distances requires some compute.


1. Load Julia and the script:
   ```
   julia --project=.
   include("scripts/analyse-runbest.jl")
   ```
