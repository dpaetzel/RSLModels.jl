# RSLModels.jl


## Generate samples for later configuring learning task generator: `scripts/genkdata.jl`


```
nix develop --impure
julia -p 20 --project=. scripts/genkdata.jl --n-iter 10000
```


This generates a CSV file such as `2023-11-23-12-23-35-578-Bym3-kdata.csv`.
Let's assume that this file's name is `$kdata_csv`.


Note that you you should probably adjust the number of processes (`-p` option to
`julia`) according to the hardware you use.


Since I'm running this on our Slurm cluster, see `slurm/genkdata.sbatch` for the
exact call I'm using.


If you have access to a Slurm cluster as well, consider to start many
medium-sized jobs (e.g. 1000 jobs with `--n-iter 2000` each) each of which will
create a CSV file.  Put the files generated into a new directory and use that
directory as `$kdata_csv` in the next step instead of a single file name.


A Slurm job corresponding to

```
nix develop . --impure --command julia -p 4 --project=. scripts/genkdata.jl --usemmap --n-iter=2000

```

running on an allocation of 4 cores of an *AMD EPYC 7502 32-Core Processor* and
100GB RAM takes around

```
sacct --job 439387 -o Elapsed --state COMPLETED | awk -F: '{ sec += ($1 * 3600) + ($2 * 60) + $3; count++ } END { avg_sec = sec / count; printf "%02d:%02d:%02d\n", avg_sec/3600, (avg_sec%3600)/60, avg_sec%60 }'
01:26:45
```


## Extract learning task generator hyperparameters: `scripts/selectgendataparams.jl`


```
nix develop --impure
julia --project=. scripts/selectgendataparams.jl $kdata_csv
```


Replace the CSV file name with the name of the file generated by
`scripts/genkdata.jl` (or the folder name containing all the CSV files to use).


## Generate learning tasks: `scripts/gendata.jl`

```
nix develop --impure
julia --project=. -e "import Pkg; Pkg.instantiate()"
julia -p 30 --project=. "scripts/gendata.jl" genall --startseed=0 --endseed=49 --prefix-fname="data" --usemmap $kdata_csv
```

Remember to adjust the parameter to `-p` to the actual number of workers you want to use.


Also, if you don't want to use mmap (because you have a lot of RAM) then don't
add the `--usemmap` flag. You probably want to do that, though, especially for
higher-than-ten dimensional data.


## Selecting a subset of the generated tasks: `scripts/drawdata.jl`


Since it's not (yet) a proper CLI app, you will have to adjust the file names at
the top of `scripts/drawdata.jl`.


```
nix develop --impure
julia --project=. scripts/drawdata.jl
```


Run the fish script generated by `drawdata.jl` in order to copy the selected
tasks into a folder.


## Check ranges of tuned parameters (i.e. the results of `run.py optparams`): `scripts/chkoptparams.jl`


For now, the interface is to adjust in the script the path to the mlruns folder
to check.


## Compute dissimilarities (can takes a long time, run this on a compute node): `scripts/computesims.jl`


This serializes a DataFrame with all the mlflow data (including the
dissimilarities) for the next step.


## Peform analysis of the run set: `scripts/analyserunbest.jl`


You probably want to run this on the server since computing the pairwise
distances requires some compute.


1. Load Julia and the script:
   ```
   julia --project=.
   include("scripts/analyse-runbest.jl")
   ```
